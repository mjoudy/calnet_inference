# ğŸ§  Neural Connectivity Pipeline (HPC + Dask + MLflow)

This repository implements a modular and reproducible pipeline for:
- Simulating calcium activity from spike trains
- Preprocessing the calcium signal
- Inferring a neural connectivity matrix

It is optimized for large datasets using Dask and runs on SLURM-based HPC clusters. MLflow is used for complete experiment tracking, including config parameters, input/output files, and derived metrics.

---

## ğŸ“ Repository Structure
```
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml            # All pipeline parameters & paths
â”‚
â”œâ”€â”€ functions/
â”‚   â”œâ”€â”€ __init__.py           # Package initialization
â”‚   â”œâ”€â”€ simulation.py         # Step 1: Calcium simulation logic
â”‚   â”œâ”€â”€ preprocessing.py      # Step 2: Feed signal estimation
â”‚   â”œâ”€â”€ inference.py          # Step 3: Connection inference
â”‚   â”œâ”€â”€ util.py              # Shared tools: file naming, config hash
â”‚   â””â”€â”€ slurm_dask.py         # Dask cluster setup for SLURM
â”‚
â”œâ”€â”€ logs/                     # Logs written by SLURM
â”‚
â”œâ”€â”€ run_pipeline.py           # Main entry point for chaining all steps
â”œâ”€â”€ mlflow_tracking.py        # Centralized MLflow logging logic
â”œâ”€â”€ job.slurm                 # Example SLURM job submission file
â”œâ”€â”€ environment.yml           # Conda environment definition
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ dry_run_setup.py      # Creates dummy spike & conn_matrix data
â”‚   â”œâ”€â”€ test_pipeline.sh      # Script to run the whole pipeline locally
â”‚   â””â”€â”€ test_pipeline.ipynb   # Notebook interface for testing & visualization
â”‚
â””â”€â”€ README.md
```

---

## ğŸ”§ Configuration

The `config/config.yaml` file should contain the following structure:

```yaml
# MLflow configuration
mlflow:
  tracking_uri: "./mlruns"  # Local path or remote URI
  experiment_name: "Default"

# Pipeline parameters
pipeline:
  save_output: true  # Whether to save intermediate outputs
  cleanup: false     # Whether to delete intermediate files

# Step-specific parameters
calcium:
  # Parameters for calcium simulation
  tau: 0.5
  dt: 0.01

preprocessing:
  # Parameters for signal preprocessing
  window_size: 100
  threshold: 0.1

inference:
  # Parameters for connectivity inference
  method: "correlation"
  threshold: 0.05
```

---

## ğŸ” Pipeline Overview

```text
spikes.zarr â†’ step1_calcium â†’ calcium.zarr
                  â†“
               step2_preprocessing â†’ preprocessed.zarr
                                          â†“
                                 step3_inference â†’ est_matrix.npy
```

Each step:
- Uses parameter-based versioning for output files
- Can optionally save/delete outputs via `save_output` flag
- Tracks all parameters and data paths using MLflow

---

## âœ… Usage Instructions

### 1. Set up your environment
```bash
conda env create -f environment.yml
conda activate calnet
```

### 2. Generate test data
```bash
python tests/dry_run_setup.py
```

### 3. Run pipeline (locally or via SLURM)
```bash
python run_pipeline.py --steps calcium preprocessing inference
```

Or use SLURM:
```bash
sbatch job.slurm
```

### 4. View MLflow UI
```bash
mlflow ui --port 5000
```

---

## ğŸ§ª Features
- Modular steps (can run individually or chained)
- Parameter-aware file naming with hashes
- Optional output cleanup
- Reproducible experiment tracking via MLflow
- Lightweight versioning built-in
- Comprehensive error handling
- Automatic log directory creation

---

## âœï¸ Author
Mohammad Joudy

Please feel free to reach out or open an issue to collaborate or suggest improvements.
