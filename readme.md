# ğŸ§  Neural Connectivity Pipeline (HPC + Dask + MLflow)

This repository implements a modular and reproducible pipeline for:
- Simulating calcium activity from spike trains
- Preprocessing the calcium signal
- Inferring a neural connectivity matrix

It is optimized for large datasets using Dask and runs on SLURM-based HPC clusters. MLflow is used for complete experiment tracking, including config parameters, input/output files, and derived metrics.

---

## ğŸ“ Repository Structure
```
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml            # All pipeline parameters & paths
â”‚
â”œâ”€â”€ functions/
â”‚   â”œâ”€â”€ simulation.py         # Step 1: Calcium simulation logic
â”‚   â”œâ”€â”€ preprocessing.py      # Step 2: Feed signal estimation
â”‚   â”œâ”€â”€ inference.py          # Step 3: Connection inference
â”‚   â”œâ”€â”€ utils.py              # Shared tools: file naming, config hash
â”‚   â””â”€â”€ slurm_dask.py         # Dask cluster setup for SLURM
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ step1_calcium.py      # SLURM-ready script for calcium simulation
â”‚   â”œâ”€â”€ step2_preprocessing.py# SLURM-ready script for preprocessing
â”‚   â””â”€â”€ step3_inference.py    # SLURM-ready script for inference
â”‚
â”œâ”€â”€ run_pipeline.py           # Main entry point for chaining all steps
â”œâ”€â”€ mlflow_tracking.py        # Centralized MLflow logging logic
â”œâ”€â”€ job.slurm                 # Example SLURM job submission file
â”œâ”€â”€ environment.yml           # Conda environment definition
â”œâ”€â”€ logs/                     # Logs written by SLURM
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ dry_run_setup.py      # Creates dummy spike & conn_matrix data
â”‚   â”œâ”€â”€ test_pipeline.sh      # Script to run the whole pipeline locally
â”‚   â””â”€â”€ test_pipeline.ipynb   # Notebook interface for testing & visualization
â”‚
â””â”€â”€ README.md
```

---

## ğŸ” Pipeline Overview

```text
spikes.zarr â†’ step1_calcium â†’ calcium.zarr
                  â†“
               step2_preprocessing â†’ preprocessed.zarr
                                          â†“
                                 step3_inference â†’ est_matrix.npy
```

Each step:
- Uses parameter-based versioning for output files
- Can optionally save/delete outputs via `save_output` flag
- Tracks all parameters and data paths using MLflow

---

## âœ… Usage Instructions

### 1. Set up your environment
```bash
conda env create -f environment.yml
conda activate neural-pipeline
```

### 2. Generate test data
```bash
python tests/dry_run_setup.py
```

### 3. Run pipeline (locally or via SLURM)
```bash
python run_pipeline.py --steps calcium preprocessing inference
```

Or use SLURM:
```bash
sbatch job.slurm
```

### 4. View MLflow UI
```bash
mlflow ui --port 5000
```

---

## ğŸ§ª Features
- Modular steps (can run individually or chained)
- Parameter-aware file naming with hashes
- Optional output cleanup
- Reproducible experiment tracking via MLflow
- Lightweight versioning built-in

---

## âœï¸ Author
Mohammad Joudy

Please feel free to reach out or open an issue to collaborate or suggest improvements.
